{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "TensorFlow [introduction to graphs](https://www.tensorflow.org/guide/intro_to_graphs)\n",
        "\n",
        "Graphs are data structures that contain a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations. They are defined in a tf.Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code for example on embedded devices, servers etc that not have built-in Python interpreters. Graphs can also be optimized to run faster.\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1Q8-9CJbEZez8oOU0K-_sHQ-tDNgj8CAQ' alt = \"graph components\" width=\"400\">\n",
        "<img src = 'https://drive.google.com/uc?id=1Q7_c8CXV8_eEvPbNyAjdUN1tXCFKUS0e' alt = \"graph\" width=\"400\">\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tNMQnzrDTAob"
      },
      "id": "tNMQnzrDTAob"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valued-lodging",
      "metadata": {
        "id": "valued-lodging"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**TensorFlow eager execution**: tensor computations are executed by Python, operation by operation, and return results back to Python.\n",
        "\n",
        "**TensorFlow graph execution**: tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf.Graph or simply a \"graph.\"\n",
        "\n",
        "Graph execution enables portability outside Python and tends to offer better performance.  \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zacewfWuTx_n"
      },
      "id": "zacewfWuTx_n"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def a_func_in_python(x, y, b):\n",
        "  x = tf.matmul(x, y)\n",
        "  x = x + b\n",
        "  return x"
      ],
      "metadata": {
        "id": "uGvdd_7VhSb7"
      },
      "id": "uGvdd_7VhSb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(a_func_in_python)"
      ],
      "metadata": {
        "id": "UToOUPfchVx3"
      },
      "id": "UToOUPfchVx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function in Python\n",
        "def a_func_in_python(x, y, b):\n",
        "  x = tf.matmul(x, y)\n",
        "  x = x + b\n",
        "  return x\n",
        "\n",
        "# A function that uses a graph version of the function above\n",
        "a_func_that_uses_graph = tf.function(a_func_in_python)\n",
        "\n",
        "# Make some tensors.\n",
        "x1 = tf.constant([[1.0, 2.0, 3.0]])\n",
        "y1 = tf.constant([[1.0], [2.0], [3.0]])\n",
        "b1 = tf.constant(4.0)\n",
        "\n",
        "# Call the two versions of the same function on the inputs\n",
        "python_way = a_func_in_python(x1, y1, b1).numpy()\n",
        "tf_function_way = a_func_that_uses_graph(x1, y1, b1).numpy()\n",
        "\n",
        "# Check if the two versions return the same output\n",
        "print(python_way)\n",
        "print(tf_function_way)"
      ],
      "metadata": {
        "id": "B2VpALYvOBoG"
      },
      "id": "B2VpALYvOBoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "tf.function applies to nested function calls\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CijOHRNvb6da"
      },
      "id": "CijOHRNvb6da"
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner function\n",
        "def inner_function(x, y, b):\n",
        "  x = tf.matmul(x, y) + b\n",
        "  return x\n",
        "\n",
        "# Outer function that is converted into a graph\n",
        "@tf.function\n",
        "def outer_function(x):\n",
        "  y = tf.constant([[1.0], [2.0], [3.0]])\n",
        "  b = tf.constant(1.0)\n",
        "  temp = inner_function(x, y, b)\n",
        "  return(temp)\n",
        "\n",
        "# Calling the outer function will result in a\n",
        "# graph that will also include the inner function's\n",
        "# graph\n",
        "outer_function(tf.constant([[1.0, 2.0, 3.0]])).numpy()"
      ],
      "metadata": {
        "id": "S6KAF3aJb_i5"
      },
      "id": "S6KAF3aJb_i5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Converting Python functions with branches into graphs.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1tdtp9vudfyZ"
      },
      "id": "1tdtp9vudfyZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_relu(x):\n",
        "  if tf.greater(x, 0):\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# Function to wrap python function into a graph\n",
        "tf_simple_relu = tf.function(simple_relu)\n",
        "\n",
        "print(\"First branch, with graph:\", tf_simple_relu(tf.constant(1)).numpy())\n",
        "print(\"Second branch, with graph:\", tf_simple_relu(tf.constant(-1)).numpy())"
      ],
      "metadata": {
        "id": "DVS7yPzUdoMp"
      },
      "id": "DVS7yPzUdoMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Polymorphism using graphs\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1Q9rMsRHWtBVSaWMsmDX6UZS8l0Cfu4eh' alt = \"graph components\" width=\"400\">\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ff0iTk_ZeLbx"
      },
      "id": "ff0iTk_ZeLbx"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def my_relu(x):\n",
        "  return tf.maximum(0., x)\n",
        "\n",
        "# Function creates a new graph for each case below\n",
        "print(my_relu(tf.constant(1.0)))\n",
        "print(my_relu([1, -1]))\n",
        "print(my_relu(tf.constant([3., -3.])))\n",
        "\n",
        "# Function does not create a new graph for each\n",
        "# case below because a graph has already been\n",
        "# created above for the given data type and shape\n",
        "print(my_relu(tf.constant(2.0)))\n",
        "print(my_relu(tf.constant([1.5, -1.5])))"
      ],
      "metadata": {
        "id": "t11Ha-JheYyA"
      },
      "id": "t11Ha-JheYyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Graph execution vs. eager execution**: the code in a tf.function can be executed both eagerly and as a graph. By default, tf.function executes its code as a graph.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SMUXPUiXfkum"
      },
      "id": "SMUXPUiXfkum"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def get_MSE(y_true, y_pred):\n",
        "  return tf.reduce_mean((y_true - y_pred)**2)\n",
        "\n",
        "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "print(y_true)\n",
        "print(y_pred)\n",
        "\n",
        "# Execution in graph mode\n",
        "print(get_MSE(y_true, y_pred))\n",
        "\n",
        "# Switch of graph mode and turn on eager execution mode\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(get_MSE(y_true, y_pred))\n",
        "\n",
        "# Get back to graph mode\n",
        "tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "lEtlY0p9fi1t"
      },
      "id": "lEtlY0p9fi1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Graph tracing only captures TensorFlow operations into a graph and not print statements etc.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "smJeM_JNgZeh"
      },
      "id": "smJeM_JNgZeh"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def get_MSE(y_true, y_pred):\n",
        "  print('Calculating MSE')\n",
        "  return tf.reduce_mean((y_true - y_pred)**2)\n",
        "\n",
        "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "\n",
        "# Execution in graph mode captures the print statement only once\n",
        "print(get_MSE(y_true, y_pred))\n",
        "print(get_MSE(y_true, y_pred))\n",
        "print(get_MSE(y_true, y_pred))\n",
        "\n",
        "# Switching to eager mode will capture the print statement all 3 times\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(get_MSE(y_true, y_pred))\n",
        "print(get_MSE(y_true, y_pred))\n",
        "print(get_MSE(y_true, y_pred))\n",
        "\n",
        "# Switch back to graph mode\n",
        "tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "r2HupaBxglnx"
      },
      "id": "r2HupaBxglnx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Graph mode corresponds to non-strict execution which means only those operations necessary to produce the observable effects are executed.\n",
        "\n",
        "In contrast, in eager execution mode, all of the program operations, needed or not, are stepped through and executed.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AETH_kyihSa1"
      },
      "id": "AETH_kyihSa1"
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([1., 2., 3.])\n",
        "\n",
        "def unused_func(x):\n",
        "  # Get index 3 will fail\n",
        "  tf.gather(x, [3]) # unused operation not connected to what function is returning\n",
        "  return x\n",
        "\n",
        "tf_unused_func = tf.function(unused_func)\n",
        "\n",
        "try:\n",
        "  print(tf_unused_func(x))\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "  # Unused operations are not run during graph execution so no error is raised.\n",
        "  print(f'{type(e).__name__}: {e}')\n",
        "\n",
        "try:\n",
        "  print(unused_func(x))\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "  # All operations are run during eager execution so an error is raised.\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ],
      "metadata": {
        "id": "-iC3KAtHhpo6"
      },
      "id": "-iC3KAtHhpo6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tf.function best practices**:\n",
        "\n",
        "- Toggle between eager and graph execution early and often with tf.config.run_functions_eagerly to pinpoint if/ when the two modes diverge.\n",
        "- Create tf.Variables outside the Python function that we want to convert into a graph and modify those variables on the inside. The same goes for objects that use tf.Variable, like tf.keras.layers, tf.keras.Models and tf.keras.optimizers.\n",
        "- Avoid writing functions that depend on outer Python variables, excluding tf.Variables and Keras objects. Learn more in Depending on Python global and free variables of the tf.function guide.\n",
        "- Prefer to write functions which take tensors and other TensorFlow types as input.\n",
        "- Include as much computation as possible under a tf.function to maximize the performance gain. For example, decorate a whole training step or the entire training loop which runs faster under tf.function."
      ],
      "metadata": {
        "id": "OusqMQE1n_Ax"
      },
      "id": "OusqMQE1n_Ax"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "tf.function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like the example below where we multiply a matrix 100 times by itself, and repeat that operation 1000 times first using eager execution and then using graph execution:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LIiCTYpbojnH"
      },
      "id": "LIiCTYpbojnH"
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)\n",
        "\n",
        "def power(x, y):\n",
        "  result = tf.eye(10, dtype=tf.dtypes.int32)\n",
        "  for _ in range(y):\n",
        "    result = tf.matmul(x, result)\n",
        "  return result\n",
        "\n",
        "print(\"Eager execution:\", timeit.timeit(lambda: power(x, 100), number=1000), \"seconds\")\n",
        "\n",
        "tf_power = tf.function(power)\n",
        "print(\"Graph execution:\", timeit.timeit(lambda: tf_power(x, 100), number=1000), \"seconds\")"
      ],
      "metadata": {
        "id": "aS9oxqKFolqo"
      },
      "id": "aS9oxqKFolqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "When is a tf.function tracing?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qfGjkRxspseL"
      },
      "id": "qfGjkRxspseL"
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(2)\n",
        "\n",
        "@tf.function\n",
        "def trace_me(x):\n",
        "  print(\"Tracing!\") # An eager-only side effect.\n",
        "  return x * x + tf.constant(2)\n",
        "\n",
        "# Tracing happens here\n",
        "print(trace_me(x))\n",
        "\n",
        "# No tracing happens here\n",
        "print(trace_me(x+1))\n",
        "\n",
        "# Retracing happens again because new Python arguments as input, such as the\n",
        "# one below will also trigger the creation of a new graph which will result\n",
        "# in extra tracing\n",
        "print(trace_me(3))"
      ],
      "metadata": {
        "id": "kQf7Ck_ept9E"
      },
      "id": "kQf7Ck_ept9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Speed-up using graph execution**: apply a 2-layer fully-connected model with 128 nodes in the hidden layer to the MNIST dataset**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2YwhWDZQwN3Y"
      },
      "id": "2YwhWDZQwN3Y"
    },
    {
      "cell_type": "code",
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[1]\n",
        "num_samples = X_train.shape[0]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Normalize the samples (images) using the training data\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ],
      "metadata": {
        "id": "XzA_IR7jyb3M"
      },
      "id": "XzA_IR7jyb3M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create source dataset from input data (this is helpful for pipelining later)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "batch_size = 100 # batch size\n",
        "# Create training batches\n",
        "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)"
      ],
      "metadata": {
        "id": "AtTH-T6myhPh"
      },
      "id": "AtTH-T6myhPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 2-layer neural network architecture with 128 nodes in the hidden layer\n",
        "# that are leaky ReLU activated\n",
        "\n",
        "# Define model\n",
        "class Model128(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(Model128, self).__init__()\n",
        "        self.layer1 = tf.keras.layers.Dense(?, dtype = 'float64',\n",
        "                                 activation = ?\n",
        "                                 )\n",
        "        self.layer2 = tf.keras.layers.Dense(?, dtype = 'float64',\n",
        "                                 activation = ?)\n",
        "\n",
        "    # Forward pass for the model\n",
        "    def call(self, inputs):\n",
        "        a = self.layer1(?)\n",
        "        a = self.?(?)\n",
        "        return ?"
      ],
      "metadata": {
        "id": "h1uTSJ8EyvR_"
      },
      "id": "h1uTSJ8EyvR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate 2-layer fully-connected model with 128 nodes in the hidden layer\n",
        "model = Model128()\n",
        "\n",
        "# Define optimizer\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-03)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Varible to store training loss per epoch\n",
        "loss_train_epoch = tf.keras.metrics.Mean()\n",
        "\n",
        "# Iterate over epochs\n",
        "nepochs = 10\n",
        "\n",
        "#@tf.function\n",
        "# User-defined function for training model\n",
        "def train_step(train_batch):\n",
        "  with tf.GradientTape() as g:\n",
        "    # Compute loss\n",
        "    yhat = model(train_batch[?])\n",
        "    loss = loss_fn(?, ?)\n",
        "\n",
        "  # Calculate gradients\n",
        "  grad = g.gradient(?, ?)\n",
        "\n",
        "  # Update model\n",
        "  opt.apply_gradients(zip(?, ?))\n",
        "\n",
        "  # Return loss\n",
        "  return(loss)\n",
        "\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "  start_time = time.time()\n",
        "  # Iterate over the batches of the dataset\n",
        "  for step, train_batch in enumerate(train_dataset):\n",
        "    # Call the training function\n",
        "    loss_train = ?(train_batch)\n",
        "    # Append training loss\n",
        "    loss_train_epoch(loss_train)\n",
        "  print('Epoch %d: time taken = %.2fs, train loss = %f'%(epoch+1, time.time() - start_time, loss_train_epoch.result()))"
      ],
      "metadata": {
        "id": "xJCwpyVl9N3A"
      },
      "id": "xJCwpyVl9N3A",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}